{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "traindf = pd.read_csv(\"KDDTrain.csv\")\n",
    "testdf = pd.read_csv(\"KDDTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5345828074269883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43075762952448543"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(traindf[traindf.attack_class == 'normal'].shape[0] / len(traindf))\n",
    "testdf[testdf.attack_class == 'normal'].shape[0] / len(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classLabel(row):\n",
    "  if row['attack_class'] == 'normal':\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>attack_class</th>\n",
       "      <th>num_learners</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>ftp_data</td>\n",
       "      <td>SF</td>\n",
       "      <td>491</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>udp</td>\n",
       "      <td>other</td>\n",
       "      <td>SF</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>neptune</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>232</td>\n",
       "      <td>8153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>normal</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>199</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normal</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
       "0         0           tcp  ftp_data   SF        491          0     0   \n",
       "1         0           udp     other   SF        146          0     0   \n",
       "2         0           tcp   private   S0          0          0     0   \n",
       "3         0           tcp      http   SF        232       8153     0   \n",
       "4         0           tcp      http   SF        199        420     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  dst_host_diff_srv_rate  \\\n",
       "0               0       0    0  ...                    0.03   \n",
       "1               0       0    0  ...                    0.60   \n",
       "2               0       0    0  ...                    0.05   \n",
       "3               0       0    0  ...                    0.00   \n",
       "4               0       0    0  ...                    0.00   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.17                         0.00   \n",
       "1                         0.88                         0.00   \n",
       "2                         0.00                         0.00   \n",
       "3                         0.03                         0.04   \n",
       "4                         0.00                         0.00   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                  0.00                      0.00                  0.05   \n",
       "1                  0.00                      0.00                  0.00   \n",
       "2                  1.00                      1.00                  0.00   \n",
       "3                  0.03                      0.01                  0.00   \n",
       "4                  0.00                      0.00                  0.00   \n",
       "\n",
       "   dst_host_srv_rerror_rate  attack_class  num_learners  label  \n",
       "0                      0.00        normal            20      0  \n",
       "1                      0.00        normal            15      0  \n",
       "2                      0.00       neptune            19      1  \n",
       "3                      0.01        normal            21      0  \n",
       "4                      0.00        normal            21      0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf['label'] = testdf.apply(classLabel,1)\n",
    "testdf.head()\n",
    "\n",
    "traindf['label'] = traindf.apply(classLabel,1)\n",
    "traindf.head()\n",
    "\n",
    "# add column for binary classification to both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(traindf):\n",
    "  categorical_columns = traindf.iloc[:,:-3].select_dtypes(include=['object']).columns.tolist()\n",
    "  #Initialize OneHotEncoder\n",
    "  encoder = OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
    "  encoder.fit(traindf[categorical_columns])\n",
    "  return encoder,categorical_columns\n",
    "\n",
    "def encode_df(encoder, df, categorical_columns):\n",
    "\n",
    "  # Apply one-hot encoding to the categorical columns\n",
    "  one_hot_encoded = encoder.transform(df[categorical_columns])\n",
    "\n",
    "  #Create a DataFrame with the one-hot encoded columns\n",
    "  #We use get_feature_names_out() to get the column names for the encoded data\n",
    "  one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names(categorical_columns))\n",
    "\n",
    "  colNanCounts = one_hot_df.isnull().sum().sum()\n",
    "\n",
    "  # Concatenate the one-hot encoded dataframe with the original dataframe\n",
    "  df_encoded = pd.concat([one_hot_df, df], axis=1)\n",
    "\n",
    "  # Drop the original categorical columns\n",
    "  df_encoded = df_encoded.drop(categorical_columns, axis=1)\n",
    "\n",
    "  return df_encoded,colNanCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "encoder, categorical_columns = get_encoder(traindf=traindf)\n",
    "\n",
    "train_encoded,train_nan_counts = encode_df(encoder,traindf,categorical_columns)\n",
    "\n",
    "# train_encoded.head()\n",
    "\n",
    "print(train_nan_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_encoded,test_nan_counts = encode_df(encoder,testdf,categorical_columns)\n",
    "\n",
    "print(test_nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NslDataSet(Dataset):\n",
    "  def __init__(self,df):\n",
    "    # self.input_df = df.iloc[:,:-3]\n",
    "    # self.label_df = df.iloc[:,-1:]\n",
    "\n",
    "    self.input_tensor = torch.tensor(df.iloc[:,:-3].values)\n",
    "    self.label_tensor = torch.tensor(df.iloc[:,-1:].values)\n",
    "\n",
    "    # print(self.input_tensor.shape)\n",
    "    # print(self.label_tensor.shape)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_tensor)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    \n",
    "    return self.input_tensor[index],self.label_tensor[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDSet = NslDataSet(train_encoded)\n",
    "testDSet = NslDataSet(test_encoded)\n",
    "\n",
    "train_loader = DataLoader(trainDSet, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(testDSet,batch_size=1,shuffle=False)\n",
    "\n",
    "numEpochs = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(1, 32, kernel_size=(9,), stride=(1,))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv1d(32, 64, kernel_size=(9,), stride=(1,))\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  (7): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (8): Dropout(p=0.253378, inplace=False)\n",
       "  (9): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = torch.nn.Conv1d(1,32,9,dtype=torch.double)\n",
    "layer2 = torch.nn.Conv1d(32,64,9,dtype=torch.double)\n",
    "\n",
    "poolingLayer = torch.nn.MaxPool1d(5)\n",
    "\n",
    "flattenLayer = torch.nn.Flatten()\n",
    "\n",
    "linearLayer = torch.nn.Linear(128,1,dtype=torch.double)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.253378)\n",
    "\n",
    "model = torch.nn.Sequential(layer,torch.nn.ReLU(),\n",
    "                            poolingLayer,\n",
    "                            layer2,torch.nn.ReLU(),\n",
    "                            poolingLayer,\n",
    "                            flattenLayer,\n",
    "                            linearLayer,dropout,\n",
    "                            torch.nn.Sigmoid())\n",
    "\n",
    "lossFcn = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.00005)\n",
    "\n",
    "model.train(True)\n",
    "\n",
    "modelPath = \"models/run2\"\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 loss per batch : 1.6512200503244976 loss per data point : 0.001625358499362861\n",
      "epoch : 1 loss per batch : 1.1904807131025696 loss per data point : 0.0011718353014115617\n",
      "epoch : 2 loss per batch : 1.177192357403407 loss per data point : 0.0011587550690864113\n",
      "epoch : 3 loss per batch : 1.1722937569688847 loss per data point : 0.001153933190954742\n",
      "epoch : 4 loss per batch : 1.2611058471561583 loss per data point : 0.0012413542985192352\n",
      "epoch : 5 loss per batch : 1.146622566451487 loss per data point : 0.0011286640648391672\n",
      "epoch : 6 loss per batch : 1.1655767030397337 loss per data point : 0.001147321340104046\n",
      "epoch : 7 loss per batch : 1.1971536127676108 loss per data point : 0.0011784036895460437\n",
      "epoch : 8 loss per batch : 1.1444684314307079 loss per data point : 0.0011265436680670284\n",
      "epoch : 9 loss per batch : 1.1990039513401864 loss per data point : 0.001180225047956174\n",
      "epoch : 10 loss per batch : 1.1292652137365111 loss per data point : 0.0011115785644807013\n",
      "epoch : 11 loss per batch : 1.1315573560077274 loss per data point : 0.0011138348070218078\n",
      "epoch : 12 loss per batch : 1.1191858124531044 loss per data point : 0.0011016570276502502\n",
      "epoch : 13 loss per batch : 1.1172824604417106 loss per data point : 0.0010997834861023563\n",
      "epoch : 14 loss per batch : 1.3010556637976 loss per data point : 0.0012806784176839674\n",
      "epoch : 15 loss per batch : 1.1090150599460602 loss per data point : 0.0010916455703469114\n",
      "epoch : 16 loss per batch : 1.1130240132123292 loss per data point : 0.001095591735041071\n",
      "epoch : 17 loss per batch : 1.1269969938769095 loss per data point : 0.0011093458696763336\n",
      "epoch : 18 loss per batch : 1.0865019541657919 loss per data point : 0.001069485066772707\n",
      "epoch : 19 loss per batch : 1.0913832625647453 loss per data point : 0.0010742899236981607\n",
      "epoch : 20 loss per batch : 1.097727818478875 loss per data point : 0.001080535110629901\n",
      "epoch : 21 loss per batch : 1.2488903153302156 loss per data point : 0.0012293300874071963\n",
      "epoch : 22 loss per batch : 1.110891159793911 loss per data point : 0.001093492286557\n",
      "epoch : 23 loss per batch : 1.3717721160229672 loss per data point : 0.0013502873027303305\n",
      "epoch : 24 loss per batch : 1.1680070595967393 loss per data point : 0.0011497136322068673\n",
      "epoch : 25 loss per batch : 1.0956946265645235 loss per data point : 0.0010785337627428173\n",
      "epoch : 26 loss per batch : 1.0892984262412393 loss per data point : 0.0010722377402611167\n",
      "epoch : 27 loss per batch : 1.0810230919421577 loss per data point : 0.001064092014962155\n",
      "epoch : 28 loss per batch : 1.0839928736464286 loss per data point : 0.0010670152836890219\n",
      "epoch : 29 loss per batch : 1.0676395970267492 loss per data point : 0.0010509181334993761\n",
      "epoch : 30 loss per batch : 1.1523243934319864 loss per data point : 0.0011342765893133156\n",
      "epoch : 31 loss per batch : 1.4073804387060087 loss per data point : 0.001385337924789797\n",
      "epoch : 32 loss per batch : 1.1086006696568564 loss per data point : 0.0010912376702741872\n",
      "epoch : 33 loss per batch : 1.054451823471608 loss per data point : 0.0010379369079920253\n",
      "epoch : 34 loss per batch : 1.0802459054845723 loss per data point : 0.001063327000865955\n",
      "epoch : 35 loss per batch : 1.0906449637830056 loss per data point : 0.0010735631882156707\n",
      "epoch : 36 loss per batch : 1.0823828596165481 loss per data point : 0.0010654304858378537\n",
      "epoch : 37 loss per batch : 1.0639110016790583 loss per data point : 0.0010472479357338733\n",
      "epoch : 38 loss per batch : 1.0916701457321585 loss per data point : 0.0010745723136766422\n",
      "epoch : 39 loss per batch : 1.537415690319087 loss per data point : 0.0015133365530674572\n",
      "epoch : 40 loss per batch : 2.7464530565398193 loss per data point : 0.002703437871694233\n",
      "epoch : 41 loss per batch : 2.0060608346044755 loss per data point : 0.0019746417366495596\n",
      "epoch : 42 loss per batch : 1.057700812871433 loss per data point : 0.001041135011439417\n",
      "epoch : 43 loss per batch : 1.0562583097203795 loss per data point : 0.0010397151008972325\n",
      "epoch : 44 loss per batch : 1.0913442364533474 loss per data point : 0.0010742515088170885\n",
      "epoch : 45 loss per batch : 1.1691057256562238 loss per data point : 0.0011507950908636912\n",
      "epoch : 46 loss per batch : 1.262506703504843 loss per data point : 0.001242733214534865\n",
      "epoch : 47 loss per batch : 1.0386328858391207 loss per data point : 0.00102236572792623\n",
      "epoch : 48 loss per batch : 1.0773988494057956 loss per data point : 0.0010605245356252423\n",
      "epoch : 49 loss per batch : 1.1500312394386702 loss per data point : 0.001132019350895788\n",
      "epoch : 50 loss per batch : 1.1154892371171978 loss per data point : 0.0010980183483963431\n",
      "epoch : 51 loss per batch : 1.063100073956998 loss per data point : 0.0010464497088317954\n",
      "epoch : 52 loss per batch : 1.062662230363117 loss per data point : 0.0010460187227820765\n",
      "epoch : 53 loss per batch : 1.0567815198278665 loss per data point : 0.0010402301164428524\n",
      "epoch : 54 loss per batch : 2.1728141355140558 loss per data point : 0.0021387833329661347\n",
      "epoch : 55 loss per batch : 1.9929050148056362 loss per data point : 0.0019616919644360213\n",
      "epoch : 56 loss per batch : 1.9270816523400545 loss per data point : 0.0018968995331552535\n",
      "epoch : 57 loss per batch : 1.8324781635721379 loss per data point : 0.001803777732394601\n",
      "epoch : 58 loss per batch : 1.772658601033961 loss per data point : 0.0017448950690085268\n",
      "epoch : 59 loss per batch : 1.707957875438533 loss per data point : 0.0016812076917623467\n",
      "epoch : 60 loss per batch : 1.5604343563642524 loss per data point : 0.00153599469877805\n",
      "epoch : 61 loss per batch : 1.3170473219166017 loss per data point : 0.0012964196130731077\n",
      "epoch : 62 loss per batch : 1.6295821946179696 loss per data point : 0.0016040595376201905\n",
      "epoch : 63 loss per batch : 2.0323427339162095 loss per data point : 0.0020005120065856173\n",
      "epoch : 64 loss per batch : 1.1183461315668604 loss per data point : 0.0011008304979185277\n",
      "epoch : 65 loss per batch : 1.9172043856778445 loss per data point : 0.0018871769650961137\n",
      "epoch : 66 loss per batch : 2.4135166803499066 loss per data point : 0.002375715973767303\n",
      "epoch : 67 loss per batch : 2.319056980238339 loss per data point : 0.002282735709632652\n",
      "epoch : 68 loss per batch : 2.2159319456624074 loss per data point : 0.00218122582825001\n",
      "epoch : 69 loss per batch : 1.7196979407675483 loss per data point : 0.0016927638831747756\n",
      "epoch : 70 loss per batch : 1.0553496201490764 loss per data point : 0.0010388206433004333\n",
      "epoch : 71 loss per batch : 1.0946187610345202 loss per data point : 0.0010774747475116137\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(numEpochs):\n",
    "\n",
    "  runningLoss = 0\n",
    "  batchNumMax = 0\n",
    "  \n",
    "\n",
    "  for batch_num,(trainInput,trainLabel) in enumerate(train_loader):\n",
    "    \n",
    "    trainInput = trainInput.to(device)\n",
    "    trainLabel = trainLabel.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(trainInput.unsqueeze(1))\n",
    "    loss = lossFcn(out,trainLabel.double())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    runningLoss += loss.item()\n",
    "\n",
    "    batchNumMax += 1\n",
    "\n",
    "  print(\"epoch :\",epoch,\"loss per batch :\",runningLoss/batchNumMax, \"loss per data point :\", runningLoss/len(train_encoded))\n",
    "\n",
    "  modelFilePath = modelPath+\"/\" +str(epoch) + \".pt\"\n",
    "  torch.save(model.state_dict(), modelFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 122])\n",
      "tensor([[0.9872]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1]])\n"
     ]
    }
   ],
   "source": [
    "TrainedModel = torch.load(\"models/run2/47.pt\")\n",
    "\n",
    "model.load_state_dict(TrainedModel)\n",
    "\n",
    "testSample,testLabel = next(iter(test_loader))\n",
    "\n",
    "print(testSample.shape)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "model.to(\"cpu\")\n",
    "\n",
    "testOut = model(testSample.unsqueeze(1))\n",
    "\n",
    "print(testOut)\n",
    "\n",
    "print(testLabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "totalTestSamples = 0\n",
    "for testBatNum,(testSample,testLabel) in enumerate(test_loader):\n",
    "  totalTestSamples += 1\n",
    "  testOut = model(testSample.unsqueeze(1))\n",
    "  if testOut < 0.5:\n",
    "    ans = 0\n",
    "  else:\n",
    "    ans = 1\n",
    "\n",
    "  if ans == testLabel.numpy():\n",
    "    correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18267\n",
      "22544\n",
      "0.8102821149751597\n"
     ]
    }
   ],
   "source": [
    "print(correct)\n",
    "print(totalTestSamples)\n",
    "\n",
    "print(correct/totalTestSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
